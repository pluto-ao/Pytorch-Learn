## 1.文章大纲

1. 张量的简介与创建（张量及各种创建方式
2. 张量的基本操作
3. 线性回归模型介绍
4. 总结

![image-20210509142422066](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509142422066.png)

## 2.张量 Tensor

### 2.1张量的简介

多维数组

![image-20210509143208192](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509143208192.png)

1. - dtype：张量的数据类型，如 torch.FloatTensor，torch.cuda.FloatTensor，用的最多的一般是 float32 和 int64(torch.long)
   - shape：张量的形状，如 (64, 3, 224, 224)
   - device：张量所在的设备，GPU/CPU，张量放在 GPU 上才能使用加速。

2. 1. data：被包装的 Tensor；
   2. grad：data 的梯度；
   3. grad_fn：fn 表示 function 的意思，记录我们创建的创建张量时用到的方法，比如说加法、乘法，这个操作在求导过程需要用到，Tensor 的 Function，是自动求导的关键；
   4. requires_grad：指示是否需要梯度，有的不需要梯度；
   5. is_leaf：指示是否是叶子节点（张量）；

3. 

### 2.2张量的创建

#### 1.直接创建

##### torch.Tensor()

**「torch.Tensor()：功能：从 data 创建 Tensor」**

![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7racicyu9LfjlW2T0FxZcuX4k9bgTF2CyicgchBquy7d1FuoS0L8Tkld3CeMew2fY0xnkAnTslkZZzukg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里的 data，就是我们的数据，可以是 list，也可以是 numpy。dtype 这个是指明数据类型，默认与 data 的一致。device 是指明所在的设备，requires_grad 是是否需要梯度，在搭建神经网络的时候需要求导的那些参数这里要设置为 true。pin_memory 是否存于锁页内存，这个设置为 False 就可以。

```python
arr = np.ones((3,3))
print('ndarry的数据类型:',arr.dtype)
t = torch.tensor(arr,device='cdue')
print(t)
##结果如下：
ndarry的数据类型：float64
tensor([[1., 1., 1.],
     [1., 1., 1.],
     [1., 1., 1.]], device='cuda:0', dtype=torch.float64)
```

##### torch.from_numpy(ndarry)

通过numpy数组创建**「torch.from_numpy(ndarry)：从 numpy 创建 tensor」**。创建的 Tensor 与原 ndarray **「共享内存」**, 当修改其中一个数据的时候，另一个也会被改动。

```python
arr = np.array([[1, 2, 3], [4, 5, 6]])
t = torch.from_numpy(arr)
print(arr, '\n',t)
arr[0, 0] = 0
print('*' * 10)
print(arr, '\n',t)
t[1, 1] = 100
print('*' * 10)
print(arr, '\n',t)
## 结果：
**********
[[  0   2   3]
[  4 100   6]] 
tensor([[  0,   2,   3],
       [  4, 100,   6]], dtype=torch.int32)
```

#### 2.依据数值

##### torch.zeros()

**「torch.zeros()：依 size 创建全 0 的张量」**

![image-20210509144731821](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509144731821.png)

layout 这个是内存中的布局形式，一般采用默认就可以。

out，表示输出张量，再把这个张量赋值给别的一个张量，但是这两个张量时一样的，指的同一个内存地址。

```python
out_t = torch.tensor([1])
t = torch.zeros((3, 3), out=out_t)

print(out_t, '\n', t)
print(id(t), id(out_t), id(t) == id(out_t))   # 这个看内存地址

## 结果：
tensor([[0, 0, 0],
       [0, 0, 0],
       [0, 0, 0]]) 
tensor([[0, 0, 0],
       [0, 0, 0],
       [0, 0, 0]])
2575719258696 2575719258696 True
```

##### torch.zeros_like（）

<font color='blue'>**「torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False)：这个是创建与 input 同形状的全 0 张量」**</font>

```python
t = torch.zeros_like(out_t)   # 这里的input要是个张量
print(t)

tensor([[0, 0, 0],
       [0, 0, 0],
       [0, 0, 0]])
```

##### torch.ones(), torch.ones_like()

**「torch.ones(), torch.ones_like()， 还可以自定义数值张量：torch.full(), torch.full_like()」**创建全1张量![image-20210509151548432](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509151548432.png)

```python
t = torch.full((3,3), 10)//fill_value要填充的值
tensor([[10., 10., 10.],
       [10., 10., 10.],
       [10., 10., 10.]])
```

##### torch.arange()

**「torch.arange()：创建等差的 1 维张量，数值区间 [start, end)，注意这是==右边开==，取不到最后的那个数。」**

![image-20210509151924924](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509151924924.png)

##### torch.linspace()

**「torch.linspace()：创建均分的 1 维张量， 数值区间 [start, end] 注意这里都是==闭区间==，和上面的区分。」**

![image-20210509152015046](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509152015046.png)

```python
t = torch.linspace(2, 10, 5)   # tensor([2, 4, 6, 8, 10])

# 那么如果不是那么正好呢？ 步长应该是多少？
t = torch.linspace(2, 10, 6)   # tensor([2, 3.6, 5.2, 6.8, 8.4, 10])

# 这个步长是怎么算的？  (end-start) / (steps-1)
```

##### **对数均分数列**

##### torch.logspace()

![image-20210509152556459](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509152556459.png)

base是以什么为底

##### torch.eye()

**「torch.eye()：创建单位对角矩阵，默认是方阵」**

![image-20210509153007150](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509153007150.png)

n,m分别是矩阵的行数和列数

### 3.依据概率分布

##### torch.normal()

**「torch.normal()：生成正态分布（高斯分布）， 这个使用的比较多」**

![image-20210509153105098](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210509153105098.png)

mean 是均值，std 是标准差。

1. - mean 为标量，std 为标量；
   - mean 为标量，std 为张量；
   - mean 为张量，std 为标量；
   - mean 为张量，std 为张量。

```python
# 第一种模式 - 均值是标量， 方差是标量 - 此时产生的是一个分布， 从这一个分部种抽样相应的个数，所以这个必须指定size，也就是抽取多少个数
t_normal = torch.normal(0, 1, size=(4,))
print(t_normal)     # 来自同一个分布

# 第二种模式 - 均值是标量， 方差是张量 - 此时会根据方差的形状大小，产生同样多个分布，每一个分布的均值都是那个标量
std = torch.arange(1, 5, dtype=torch.float)
print(std.dtype)
t_normal2 = torch.normal(1, std)
print(t_normal2)        # 也产生来四个数，但是这四个数分别来自四个不同的正态分布，这些分布均值相等

# 第三种模式 - 均值是张量，方差是标量 - 此时也会根据均值的形状大小，产生同样多个方差相同的分布，从这几个分布中分别取一个值作为结果
mean = torch.arange(1, 5, dtype=torch.float)
t_normal3 = torch.normal(mean, 1)
print(t_normal3)     # 来自不同的分布，但分布里面方差相等

# 第四种模式 - 均值是张量， 方差是张量 - 此时需要均值的个数和方差的个数一样多，分别产生这么多个正太分布，从这里面抽取一个值
mean = torch.arange(1, 5, dtype=torch.float)
std = torch.arange(1, 5, dtype=torch.float)
t_normal4 = torch.normal(mean, std)
print(t_normal4)          # 来自不同的分布，各自有自己的均值和方差
```

##### torch.randn() 标准正态分布

<font color='RED'>**「标准正态分布：torch.randn(), torch.randn_like()」**</font>

![image-20210510102242514](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510102242514.png)

##### torch.rand() 均匀分布

**「生成均匀分布：torch.rand(), rand_like()  在 [0,1) 生成均匀分布 torch.randint(), torch.randint_like()：区间 [low,hight) 生成整数均匀分布」**

![image-20210510102334436](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510102334436.png)

##### torch.randperm(n)   .bernoulli(input)

- **「torch.randperm(n)：生成从 0 - n-1 的随机排列, n 是张量的长度, 经常用来生成一个乱序索引。」**
- **「torch.bernoulli(input)：以 input 为概率，生成伯努利分布 (0-1 分布，两点分布）， input：概率值」**

![image-20210510102918413](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510102918413.png)

## 3.张量的操作

### 3.1基本操作

#### 1.张量的拼接

##### .cat    .stack

- **「torch.cat(tensors, dim=0, out=None)：将张量按维度 dim 进行拼接, tensors 表示张量序列， dim 要拼接的维度」**
- **「torch.stack(tensors, dim=0, out=None)：在新创建的维度 dim 上进行拼接， tensors 表示张量序列， dim 要拼接的维度」**

```python
# 张量的拼接
t = torch.ones((2, 3))
print(t)

t_0 = torch.cat([t, t], dim=0)       # 行拼接
t_1 = torch.cat([t, t], dim=1)    # 列拼接
print(t_0, t_0.shape)
print(t_1, t_1.shape)

# 结果：
tensor([[1., 1., 1.],
     [1., 1., 1.]])
tensor([[1., 1., 1.],
     [1., 1., 1.],
     [1., 1., 1.],
     [1., 1., 1.]]) torch.Size([4, 3])
tensor([[1., 1., 1., 1., 1., 1.],
     [1., 1., 1., 1., 1., 1.]]) torch.Size([2, 6])
```

*.cat浮点数类型拼接才可以，long 类型拼接会报错。*

```python
t_stack = torch.stack([t,t,t], dim=0) //横向看
print(t_stack)
print(t_stack.shape)

t_stack1 = torch.stack([t, t, t], dim=1)  //纵向看
print(t_stack1)
print(t_stack1.shape)

## 结果：
tensor([[[1., 1., 1.],
        [1., 1., 1.]],

       [[1., 1., 1.],
        [1., 1., 1.]],

       [[1., 1., 1.],
        [1., 1., 1.]]])
torch.Size([3, 2, 3])
tensor([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]],

       [[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]]])
torch.Size([2, 3, 3])
```

#### 2.张量的切分

##### .chunk()

**torch.chunk(input, chunks, dim=0)：将张量按维度 dim 进行平均切分，返回值是张量列表，注意，如果不能整除， 最后一份张量小于其他张量。chunks 代表要切分的维度。**

```python
a = torch.ones((2, 7))  # 7
list_of_tensors = torch.chunk(a, dim=1, chunks=3)   # 第一个维度切成三块， 那么应该是(2,3), (2,3), (2,1)  因为7不能整除3，所以每一份应该向上取整，最后不够的有多少算多少
print(list_of_tensors)
for idx, t in enumerate(list_of_tensors):
   print("第{}个张量：{}, shape is {}".format(idx+1, t, t.shape))

## 结果：
(tensor([[1., 1., 1.],
       [1., 1., 1.]]), tensor([[1., 1., 1.],
       [1., 1., 1.]]), tensor([[1.],
       [1.]]))
第1个张量：tensor([[1., 1., 1.],
       [1., 1., 1.]]), shape is torch.Size([2, 3])
第2个张量：tensor([[1., 1., 1.],
       [1., 1., 1.]]), shape is torch.Size([2, 3])
第3个张量：tensor([[1.],
       [1.]]), shape is torch.Size([2, 1])
```

##### .split() 指定长度

**「torch.split(tensor, split_size_or_sections, dim=0)：这个也是将张量按维度 dim 切分，但是这个更加强大，可以指定切分的长度，split_size_or_sections 为 int 时表示每一份的长度， 为 list 时，按 list 元素切分」**

```python
# split
t = torch.ones((2, 5))

list_of_tensors = torch.split(t, [2, 1, 2], dim=1)  # [2 , 1, 2]， 这个要保证这个list的大小正好是那个维度的总大小，这样才能切
for idx, t in enumerate(list_of_tensors):
   print("第{}个张量：{}, shape is {}".format(idx+1, t, t.shape))

## 结果
第1个张量：tensor([[1., 1.],
       [1., 1.]]), shape is torch.Size([2, 2])
第2个张量：tensor([[1.],
       [1.]]), shape is torch.Size([2, 1])
第3个张量：tensor([[1., 1.],
       [1., 1.]]), shape is torch.Size([2, 2])
```

.chunk 切分的规则就是提供张量，切分的维度和几份， 比如三份，先计算每一份的大小，也就是这个维度的长度除以三，然后上取整，就开始沿着这个维度切，最后不够一份大小的，也就那样了。所以长度为 7 的这个维度，3 块，每块 7/3 上取整是 3，然后第一块 3，第二块是 3，第三块 1。这样切 .split 这个函数的功能更加强大，它可以指定每一份的长度，只要传入一个列表即可，或者也有一个整数，表示每一份的长度，这个就根据每一份的长度先切着，看看能切几块算几块。不过列表的那个好使，可以自己指定每一块的长度，但是注意一下，这个长度的总和必须是维度的那个总长度才用办法切。

#### 3.张量的索引

##### .index_sekect()

**「torch.index_select(input, dim, index, out=None)：在维度 dim 上，按 index 索引数据，返回值，以 index 索引数据拼接的张量。」**

```python
t = torch.randint(0, 9, size=(3, 3))     #  从0-8随机产生数组成3*3的矩阵
print(t)
idx = torch.tensor([0, 2], dtype=torch.long)   # 这里的类型注意一下，要是long类型
t_select = torch.index_select(t, dim=1, index=idx)  #第0列和第2列拼接返回
print(t_select)

## 结果：
tensor([[3, 7, 3],
     [4, 3, 7],
     [5, 8, 0]])
tensor([[3, 3],
     [4, 7],
     [5, 0]])
```

##### .masked_select()

**「torch.masked_select(input, mask, out=None)：按 mask 中的 True 进行索引，返回值：一维张量。input 表示要索引的张量，mask 表示与 input 同形状的布尔类型的张量。这种情况在选择符合某些特定条件的元素的时候非常好使」**

```python
mask = t.ge(5)   # le表示<=5, ge表示>=5 gt >5  lt <5
print("mask：\n", mask)
t_select1 = torch.masked_select(t, mask)   # 选出t中大于5的元素
print(t_select1)

## 结果：
mask：
tensor([[False,  True, False],
     [False, False,  True],
     [ True,  True, False]])
tensor([7, 7, 5, 8])
```

- .index_select：按照索引查找  需要先指定一个 Tensor 的索引量，然后指定类型是 long 的
- .masked_select：就是按照值的条件进行查找，需要先指定条件作为 mask

#### 4.张量的变换

**「torch.reshape(input, shape)：变换张量的形状，这个很常用，input 表示要变换的张量，shape表示新张量的形状。但注意，当张量在内存中是连续时，新张量与input共享数据内存」**

```python
# torch.reshape
t = torch.randperm(8)       # randperm是随机排列的一个函数
print(t)

t_reshape = torch.reshape(t, (-1, 2, 2))    # -1的话就是根据后面那两个参数，计算出-1这个值，然后再转
print("t:{}\nt_reshape:\n{}".format(t, t_reshape))

t[0] = 1024
print("t:{}\nt_reshape:\n{}".format(t, t_reshape))
print("t.data 内存地址:{}".format(id(t.data)))
print("t_reshape.data 内存地址:{}".format(id(t_reshape.data))) # 这个注意一下，两个是共内存的

## 结果：
tensor([2, 4, 3, 1, 5, 6, 7, 0])
t:tensor([2, 4, 3, 1, 5, 6, 7, 0])
t_reshape:
tensor([[[2, 4],
        [3, 1]],

       [[5, 6],
        [7, 0]]])
t:tensor([1024,    4,    3,    1,    5,    6,    7,    0])
t_reshape:
tensor([[[1024,    4],
        [   3,    1]],

       [[   5,    6],
        [   7,    0]]])
t.data 内存地址:1556953167336
t_reshape.data 内存地址:1556953167336
```

**「torch.transpose(input, dim0, dim1)：交换张量的两个维度, 矩阵的转置常用， 在图像的预处理中常用， dim0 要交换的维度， dim1 表示要交换的问题」**

```python
# torch.transpose
t = torch.rand((2, 3, 4))      # 产生0-1之间的随机数
print(t)
t_transpose = torch.transpose(t, dim0=0, dim1=2)    # c*h*w     h*w*c， 这表示第0维和第2维进行交换
print("t shape:{}\nt_transpose shape：{}".format(t.shape, t_transpose.shape))

## 结果：
tensor([[[0.7480, 0.5601, 0.1674, 0.3333],
        [0.4648, 0.6332, 0.7692, 0.2147],
        [0.7815, 0.8644, 0.6052, 0.3650]],

       [[0.2536, 0.1642, 0.2833, 0.3858],
        [0.8337, 0.6173, 0.3923, 0.1878],
        [0.8375, 0.2109, 0.4282, 0.4974]]])
t shape:torch.Size([2, 3, 4])
t_transpose shape：torch.Size([4, 3, 2])
tensor([[[0.7480, 0.2536],
        [0.4648, 0.8337],
        [0.7815, 0.8375]],

       [[0.5601, 0.1642],
        [0.6332, 0.6173],
        [0.8644, 0.2109]],

       [[0.1674, 0.2833],
        [0.7692, 0.3923],
        [0.6052, 0.4282]],

       [[0.3333, 0.3858],
        [0.2147, 0.1878],
        [0.3650, 0.4974]]])
```

**「torch.t(input)：2 维张量的转置， 对矩阵而言，相当于 torch.transpose(inpuot, 0,1)」**

**「torch.squeeze(input, dim=None, out=None)：压缩长度为 1 的维度， dim 若为 None，移除所有长度为 1 的轴，若指定维度，当且仅当该轴长度为 1 时可以被移除」**

```python
# torch.squeeze
t = torch.rand((1, 2, 3, 1))
t_sq = torch.squeeze(t)
t_0 = torch.squeeze(t, dim=0)
t_1 = torch.squeeze(t, dim=1)
print(t.shape)        # torch.Size([1, 2, 3, 1])
print(t_sq.shape)     # torch.Size([2, 3])
print(t_0.shape)     # torch.Size([2, 3, 1])
print(t_1.shape)     # torch.Size([1, 2, 3, 1])
```

**「torch.unsqueeze(input, dim, out=None)：依据 dim 扩展维度」**

### 3.2 **张量的数学运算**

加减乘除，对数指数幂函数，三角函数

![image-20210510125433404](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510125433404.png)

**torch.add(input, alpha=1, other, out=None)：逐元素计算input+alpha * other。注意人家这里有个 alpha，叫做乘项因子。类似权重的个东西。**这个东西让计算变得更加简洁， 比如线性回归我们知道有个 y = wx + b， 在这里直接一行代码torch.add(b, w, x) 就搞定。类似的还有两个方法：

![image-20210510125701858](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510125701858.png)

```python
t_0 = torch.randn((3, 3))
t_1 = torch.ones_like(t_0)
t_add = torch.add(t_0, 10, t_1)

print("t_0:\n{}\nt_1:\n{}\nt_add_10:\n{}".format(t_0, t_1, t_add))

## 结果：
t_0:
tensor([[-0.4133,  1.4492, -0.1619],
        [-0.4508,  1.2543,  0.2360],
        [ 1.0054,  1.2767,  0.9953]])
t_1:
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
t_add_10:
tensor([[ 9.5867, 11.4492,  9.8381],
        [ 9.5492, 11.2543, 10.2360],
        [11.0054, 11.2767, 10.9953]])
```

## 4.线性回归模型

![image-20210510130035709](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510130035709.png)

![image-20210510130022226](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510130022226.png)

```python
# 首先我们得有训练样本X，Y， 这里我们随机生成
x = torch.rand(20, 1) * 10
y = 2 * x + (5 + torch.randn(20, 1))

# 构建线性回归函数的参数
w = torch.randn((1), requires_grad=True)
b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度

for iteration in range(100):
 # 前向传播
 wx = torch.mul(w, x)
 y_pred = torch.add(wx, b)
 
 # 计算loss
 loss = (0.5 * (y-y_pred)**2).mean()
 
 # 反向传播
 loss.backward()
 
 # 更新参数
 b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=
 w.data.sub_(lr * w.grad)

 # 梯度清零
 w.grad.data.zero_()
 b.grad.data.zero_()

print(w.data, b.data)
```

![image-20210510155132512](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510155132512.png)