# 1.大纲

- 计算图与 Pytorch 的动态图机制（计算图的概念，动态图与静态图的差异和搭建过程）
- Pytorch 的自动求导机制
- 基于前面所学玩一个逻辑回归
- 总结梳理![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7rac9dtbepcdBibVo1KDD3icTYwGqtSApwgEgDM9oXpEDkJUqicLdp97jRVibrwdj4kGF5Azslic4Qh3YvaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

# 2.计算图

## 2.1计算图

计算图是用来**「描述运算」**的有向五环图。主要有两个因素：节点和边。其中节点表示数据，如向量，矩阵，张量，而边表示运算，如加减乘除，卷积等。

![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7rac9dtbepcdBibVo1KDD3icTYw1nMUhR26LJIw6TfmlDHHUNPCRNLDXT6rJU6cFvpWVn5g8Gxv2PgmEg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

```python
w = torch.tensor([1.], requires_grad=True)
x = torch.tensor([2.], requires_grad=True)

a = torch.add(w, x)
b = torch.add(w, 1)
y = torch.mul(a, b)

y.backward()
print(w.grad)   # tensor([5.])
```

梯度除叶子节点外，其余默认清空（保留可在反向传播前加a.retain_grad()）

grad_fn：记录创建该张量时所用的方法（函数）**「用于梯度的求导」**

## 2.2动态图

根据计算图的搭建方式，可以将计算图分为动态图和静态图。

- 静态图：先搭建图，后运算。高效，不灵活（TensorFlow）
- 动态图：运算与搭建同时进行。灵活，易调节（Pytorch）

# 3.自动求导机制

自动求梯度

![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7rac9dtbepcdBibVo1KDD3icTYwVEEuiapZYiba9b52G5q3z5oAyJtoJIUoIYWOGx5ycymQUM6GE4Zqh8Ww/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- tensors 表示用于求导的张量，如 loss。
- retain_graph 表示保存计算图， 由于 Pytorch 采用了动态图机制，在每一次反向传播结束之后，计算图都会被释放掉。如果我们不想被释放，就要设置这个参数为 True
- create_graph 表示创建导数计算图，用于高阶求导。
- grad_tensors 表示多梯度权重。如果有多个 loss 需要计算梯度的时候，就要设置这些 loss 的权重比例。

**多个梯度**

![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7rac9dtbepcdBibVo1KDD3icTYwmQdqicQ2rZOOgwzCd487SHXceRGz299PtFIvYnoSicANiaJNCZ1qo44Fg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

```python
grad_tensors = torch.tensor([1., 1.])
loss.backward(gradient=grad_tensors)    
print(w.grad)   #  这时候会是tensor([7.])   5+2

grad_tensors = torch.tensor([1., 2.])
loss.backward(gradient=grad_tensors)    
print(w.grad)   #  这时候会是tensor([9.])   5+2*2
```

**torch.autograd.grad()**

求取梯度，实现高阶的求导

![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7rac9dtbepcdBibVo1KDD3icTYwDibJRj2Mehcd8ibrZYF7AHdd2FSmwwdsvPyZqBnwW02SWicCS0ib89YDmA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- outputs: 用于求导的张量，如 loss
- inputs: 需要梯度的张量，如上面例子的 w
- create_graph: 创建导数计算图，用于高阶求导
- retain_graph: 保存计算图
- grad_outputs: 多梯度权重

```python
x = torch.tensor([3.], requires_grad=True)
y = torch.pow(x, 2)   # y=x^2

# 一次求导
grad_1 = torch.autograd.grad(y, x, create_graph=True)   # 这里必须创建导数的计算图， grad_1 = dy/dx = 2x
print(grad_1)   # (tensor([6.], grad_fn=<MulBackward0>),) 这是个元组，二次求导的时候我们需要第一部分

# 二次求导
grad_2 = torch.autograd.grad(grad_1[0], x)    # grad_2 = d(dy/dx) /dx = 2
print(grad_2)  # (tensor([2.]),)
```

允许对多个自变量求导数：

```python
x1 = torch.tensor(1.0,requires_grad = True) # x需要被求导
x2 = torch.tensor(2.0,requires_grad = True)

y1 = x1*x2
y2 = x1+x2


# 允许同时对多个自变量求导数
(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = True)
print(dy1_dx1,dy1_dx2)        # tensor(2.) tensor(1.)

# 如果有多个因变量，相当于把多个因变量的梯度结果求和
(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])
print(dy12_dx1,dy12_dx2)        # tensor(3.) tensor(2.)
```

### **注意：**

梯度不自动清零：就是每一次反向传播，梯度都会叠加上去

```python
w = torch.tensor([1.], requires_grad=True)
x = torch.tensor([2.], requires_grad=True)

for i in range(4):
    a = torch.add(w, x)
    b = torch.add(w, 1)
    y = torch.mul(a, b)

    y.backward()
    print(w.grad)

## 结果：
tensor([5.])
tensor([10.])
tensor([15.])
tensor([20.])
```

1. 梯度不会自动清零**「要手动的清除梯度」**添加 w.grad.zero_()     _代表原位操作

2. 依赖于叶子节点的节点，requires_grad 默认为 True

3. 叶子节点不可执行 in-place（这个 in-place 就是原位操作）

   在原始内存当中去改变这个数据。有没有新建对象

```python
a = torch.ones((1,))
print(id(a), a)    # 1407221517192 tensor([1.])

# 我们执行普通的a = a+1操作
a = a + torch.ones((1,))
print(id(a), a)    # 1407509388808 tensor([2.])  
# 会发现上面这两个a并不是同一个内存空间

# 那么执行原位操作呢？
a = torch.ones((1,))
print(id(a), a)    # 2112218352520 tensor([1.])
a += torch.ones((1,))
print(id(a), a)   # 2112218352520 tensor([2.])
```

# 4.逻辑回归模型

## 梳理

![image-20210510195631508](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510195631508.png)

- 线性回归：自变量是 X， 因变量是 y， 关系：y=wx + b， 图像是一条直线。是分析自变量 x 和因变量 y (标量)之间关系的方法。注意这里的线性是针对于 w 进行说的， 一个 w 只影响一个 x。决策边界是一条直线
- 逻辑回归：自变量是 X， 因变量是 y， 只不过这里的 y 变成了概率

![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7rac9dtbepcdBibVo1KDD3icTYw2k4DB6jLhY2sDZtBXSs4tS4Er863NbCCuQrCsq4kFUJ4NA1YuW7Q1Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![image-20210510200453862](C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20210510200453862.png)

## 模型

### 步骤

![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7rac9dtbepcdBibVo1KDD3icTYwAgtLkGYkS6qnjFicJGC4fPHqqXXtonETLwIeicq1sY6R4iaaN4qriaw1kQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1. 数据模块（数据采集，清洗，处理等）
2. 建立模型（各种模型的建立）
3. 损失函数的选择（根据不同的任务选择不同的损失函数），有了loss就可以求取梯度
4. 得到梯度之后，我们会选择某种优化方式去进行优化
5. 然后迭代训练

### 实现

1.数据生成 这里我们使用随机生成的方式，生成 2 类样本（用 0 和 1 表示）， 每一类样本 100 个， 每一个样本两个特征。

```python
"""数据生成"""
torch.manual_seed(1)

sample_nums = 100
mean_value = 1.7
bias = 1

n_data = torch.ones(sample_nums, 2)
x0 = torch.normal(mean_value*n_data, 1) + bias  # 类别0  数据shape=(100,2)
y0 = torch.zeros(sample_nums)   # 类别0， 数据shape=(100, 1)
x1 = torch.normal(-mean_value*n_data, 1) + bias   # 类别1， 数据shape=(100,2)
y1 = torch.ones(sample_nums)    # 类别1  shape=(100, 1)

train_x = torch.cat([x0, x1], 0)
train_y = torch.cat([y0, y1], 0)
```

2.建立模型 这里我们使用两种方式建立我们的逻辑回归模型，一种是 Pytorch 的 sequential 方式，这种方式就是简单，易懂，就类似于搭积木一样，一层一层往上搭。另一种方式是继承 nn.Module 这个类搭建模型，这种方式非常灵活，能够搭建各种复杂的网络。

```python
"""建立模型"""
class LR(torch.nn.Module):
    def __init__(self):
        super(LR, self).__init__()
        self.features = torch.nn.Linear(2, 1)  # Linear 是module的子类，是参数化module的一种，与其名称一样，表示着一种线性变换。输入2个节点，输出1个节点
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.features(x)
        x = self.sigmoid(x)

        return x

lr_net = LR()     # 实例化逻辑回归模型
```

另外一种方式，Sequential 的方法：

```python
lr_net = torch.nn.Sequential(
    torch.nn.Linear(2, 1),
    torch.nn.Sigmoid()
)
```

3.选择损失函数 关于损失函数的详细介绍，后面会专门整理一篇， 这里我们使用二进制交叉熵损失

```python
"""选择损失函数"""
loss_fn = torch.nn.BCELoss()
```

4.选择优化器 优化器的知识，后面也是单独会有一篇文章，这里我们就用比较常用的 SGD 优化器。关于这些参数，这里不懂没有问题，后面会单独的讲， 这也就是为啥要系统学习一遍 Pytorch 的原因， 就比如这个优化器，我们虽然知道这里用了 SGD，但是我们可能并不知道还有哪些常用的优化器，这些优化器通常用在什么情况下。

```python
"""选择优化器"""
lr = 0.01
optimizer = torch.optim.SGD(lr_net.parameters(), lr=lr, momentum=0.9)
```

5.迭代训练模型 这里就是我们的迭代训练过程了，基本上也比较简单，在一个循环中反复训练，先前向传播，然后计算梯度，然后反向传播，更新参数，梯度清零。

```python
"""模型训练"""
for iteration in range(1000):

    # 前向传播
    y_pred = lr_net(train_x)

    # 计算loss
    loss = loss_fn(y_pred.squeeze(), train_y)

    # 反向传播
    loss.backward()

    # 更新参数
    optimizer.step()

    # 清空梯度
    optimizer.zero_grad()

    # 绘图
    if iteration % 20 == 0:

        mask = y_pred.ge(0.5).float().squeeze()  # 以0.5为阈值进行分类
        correct = (mask == train_y).sum()  # 计算正确预测的样本个数
        acc = correct.item() / train_y.size(0)  # 计算分类准确率

        plt.scatter(x0.data.numpy()[:, 0], x0.data.numpy()[:, 1], c='r', label='class 0')
        plt.scatter(x1.data.numpy()[:, 0], x1.data.numpy()[:, 1], c='b', label='class 1')

        w0, w1 = lr_net.features.weight[0]
        w0, w1 = float(w0.item()), float(w1.item())
        plot_b = float(lr_net.features.bias[0].item())
        plot_x = np.arange(-6, 6, 0.1)
        plot_y = (-w0 * plot_x - plot_b) / w1

        plt.xlim(-5, 7)
        plt.ylim(-7, 7)
        plt.plot(plot_x, plot_y)

        plt.text(-5, 5, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color': 'red'})
        plt.title("Iteration: {}\nw0:{:.2f} w1:{:.2f} b: {:.2f} accuracy:{:.2%}".format(iteration, w0, w1, plot_b, acc))
        plt.legend()

        plt.show()
        plt.pause(0.5)

        if acc > 0.99:
            break
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/210IyDic7rac9dtbepcdBibVo1KDD3icTYw0K3ibicOADcbc2YdvSccwnBP6Ns2lNqQ0A8HvXtbxUyldYOw2kZkTLeQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)